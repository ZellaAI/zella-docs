---
title: 'Using Zella Router'
description: "Let's start by sending your first chat completion request"
---

In this step, we'll utilize the platform and router `zella-101-test` configured in the earlier steps by initiating our first chat completion request. 
Zella Gateway offers several methods to send a request, providing you with flexible options to suit your needs.

## REST API
```bash Curl
curl --request POST \
--url https://gateway.zella.ai/v1/chat/completions \
--header 'content-type: application/json' \
--header 'Authorization: Bearer api_h8j3g7cc68a89sc2eod' \
--data '{
"model": {
    "name": "zella-101-test"
},
"query": {
    "messages": [
        {
            "role": "system",
            "content": "You are a helpful assistant to understand how Zella works."
        },
        {
            "role": "user",
            "content": "How do I manage my models in Zella?"
        }
    ]
},
"response": {
    "stream": false
}
}'
````

## Python SDKs

With Zella, you have the flexibility to use not just our Python SDK, but also the OpenAI and Langchain Python SDKs for sending requests through the Zella Gateway. 
While we recommend using the Zella SDK for full access to all features and support, 
you can easily integrate with the OpenAI or Langchain SDKs if you prefer minimal changes to your existing workflow. This allows for a swift and convenient start.

#### Install Zella Python SDK

```bash
pip install zella
```

#### Make chat completions request in python
<CodeGroup>
```python Zella SDK
from zella import ZellaAI

zella_ai = ZellaAI(api_key="*****************")

model = {
    "name": "zella-101-test"
}
query = {
    "messages": [ {
        "role": "system",
        "content": "You are a helpful assistant to understand how Zella works."
    },
    {
        "role": "user",
        "content": "How do I add fallbacks in Zella?"
    }]
}
response = {
    "stream": "true"
}
response = zella_ai.chat.completions.create(user=user, model=model, query=query, response=response)
```

```python OpenAI SDK
from openai import OpenAI

client = OpenAI(
    api_key="*********", # Zella API Key
    base_url="https://gateway.zella.ai/v1/openai"
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How do I add fallbacks in Zella?",
        }
    ],
    model="zella-101-test", # Pass our configured model name here.
    stream=False,
)
print(chat_completion)
```

```python Langchain SDK
from langchain.chat_models import ChatOpenAI

client = ChatOpenAI(
            openai_api_base="https://gateway.zella.ai/v1/openai",
            model_name="zella-101-test",  # Pass our configured model name here.
            api_key="******************",  # Zella API Key
            streaming=False
        )
chat_completion = client([HumanMessage(content="How do I add fallbacks in Zella?")])
print(chat_completion)
```
</CodeGroup>


## Next steps

Setup a prompt: [Setup Prompt](/zella-101/prompts/setup-prompt).