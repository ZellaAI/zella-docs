---
title: "LLM Evaluator"
description: "The LLM Evaluator in Zella AI represents a groundbreaking approach to evaluation, leveraging the capabilities of language models to assess AI applications. This specialized evaluator empowers developers to evaluate responses based on the output generated by sophisticated language models, enabling a comprehensive assessment of AI interactions.
"
---

## What is LLM Evaluator?
The LLM Evaluator harnesses the power of language models to evaluate the quality, accuracy, and relevance of AI responses. Unlike traditional evaluators, which rely solely on predefined criteria, the LLM Evaluator dynamically assesses responses based on the understanding and context provided by language models.

## Key Features of the LLM Evaluator
- **Dynamic Assessment**: The LLM Evaluator dynamically assesses responses based on the context and understanding provided by language models.
- **Comprehensive Evaluation**: Evaluation considers various factors such as relevance, coherence, and accuracy, providing a comprehensive assessment of AI interactions.

## Types of LLM Evaluators in Zella AI

Zella AI offers three distinct types of LLM (Language Model) Evaluators, each tailored to address specific evaluation needs and scenarios:

1. ### Few Shot Similarity Evaluator
The Few Shot Similarity Evaluator enables developers to assess response similarity by leveraging custom prompts and benchmark datasets. This evaluator empowers developers to evaluate how well AI responses align with expected outcomes across various contexts, facilitating nuanced assessment and refinement of AI applications.

2. ### LLM Similarity Evaluator
The LLM Similarity Evaluator evaluates response similarity using original query prompts. By analyzing the similarity between the generated responses and the input queries, this evaluator provides insights into the relevance and accuracy of AI interactions. Developers can leverage this evaluator to ensure that AI responses align closely with user queries and expectations.

3. ### LLM Custom Evaluator
The LLM Custom Evaluator offers unparalleled flexibility by allowing developers to customize model usage for diverse evaluation needs. Whether fine-tuning parameters, adjusting model configurations, or exploring specialized use cases, this evaluator empowers developers to tailor evaluation methodologies to match specific requirements, driving innovation and excellence in AI development.

Each type of LLM Evaluator in Zella AI provides unique capabilities and functionalities, enabling developers to conduct comprehensive evaluations and optimize AI applications for optimal performance and effectiveness.

## Create Evaluator Page

Navigate to `Zella AI -> Dashboard -> Analysis -> Evaluation`. For first-time users, a selection of templates awaits, catering to diverse needs. Opt for the template labeled with `LLM` as the type; alternatively, locate the button at the top right corner marked Create Evaluation, and proceed to select `LLM`.

The initial step involves bestowing the evaluator with a name, description, and a subtype.

<img src="/images/evaluators/llm/step-1.png" />

## Configuring LLM evaluator

Zella AI provides three subtypes of LLM evaluators, each requiring slightly different configurations.

1. ### Few Shot Similarity Evaluator
Select a model for evaluation, input prompts, and choose a dataset. Additionally, provide code to determine the output. Below is an example of the required code:

```js
global.correctnessScore = global.similarity
if (global.correctnessScore > 0.90) {
  global.result = true
} else {
  global.result = false
}
```
<Warning>
Ensure the code includes the necessary variables:
1. `result`: A boolean field indicating the evaluation outcome.
2. `correctnessScore`: A floating-point field ranging from 0 to 1, indicating the correctness of the evaluation.
3. `feedbacks`: A string providing feedback on the evaluation.
</Warning>
<img src="/images/evaluators/llm/few-step-2.png" />

2. ### LLM Similarity Evaluator

Choose a model for evaluation and input prompts. This type does not require a dataset. Provide code similar to the Few Shot Similarity Evaluator. Below is an example:

```js
global.correctnessScore = global.similarity
if (global.correctnessScore > 0.90) {
  global.result = true
} else {
  global.result = false
}
```

<Warning>
Ensure the code includes all required variables as in the **Few Shot Similarity Evaluator**.
</Warning>

<img src="/images/evaluators/llm/sim-step-2.png" />


3. ### LLM Custom Evaluator
Configuration for this type mirrors the LLM Similarity Evaluator. Select a model for evaluation, input prompts, and provide code for output determination, similar to the **Few Shot Similarity Evaluator**. Below is an example:

```js
global.correctnessScore = global.similarity
if (global.correctnessScore > 0.90) {
  global.result = true
} else {
  global.result = false
}
```

<Warning>
Ensure the code includes all necessary variables as in the **Few Shot Similarity Evaluator**.
</Warning>

<img src="/images/evaluators/llm/sim-step-2.png" />