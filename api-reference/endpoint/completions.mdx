---
title: "Completions"
api: "POST /v1/completions"
---

# Completion Request

## Request Headers

<ParamField body="Authorization" type="string" required="true">
  Zella Access key generated on dashboard. [See how to generate access key](/zella-101/setup-access#get-your-access-key).
</ParamField>


## Request Body Parameters

<ParamField body="user" type="string">
  The unique identifier of the user.
</ParamField>

<ParamField body="model" type="Object" required="true">
  The model configuration used for the completion. [See how to choose model configuration](/models).
    <Expandable title="properties">
        <ParamField path="platform" type="string">
            The AI platform used for the model. This is an optional field, only required when using models which
            are not listed in Zella dashboard.
        </ParamField>
        <ParamField path="name" type="string" required="true">
            The name of the AI model. This should be a valid model available on the specified platform.
        </ParamField>
        <ParamField path="api_key" type="string">
            Platform API key. Not required if platform already configured in the dashboard.
        </ParamField>
        <ParamField path="config" type="Object">
            Contains any additional parameters to be used by the model. Check your platform for the exact lower/upper limit of these configs.
            <Expandable title="properties">
                <ParamField path="frequency_penalty" type="number">
                    The frequency penalty to be used by the model. Must be a number between -2 and 2. Default is 0.
                </ParamField>
                <ParamField path="logit_bias" type="Object">
                    The logit bias to be used by the model. Must be an object with keys as tokens and values as numbers. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
                    <Expandable title="properties">
                        <ParamField path="<token>" type="number" required="true">
                            Token must be a positive integer. The associated bias value from -100 to 100.
                        </ParamField>
                    </Expandable>
                </ParamField>
                <ParamField path="max_tokens" type="number">
                    The maximum number of tokens to be generated by the model. Must be an integer greater than 0.
                </ParamField>
                <ParamField path="min_tokens" type="number">
                    The minimum number of tokens to be generated by the model.
                </ParamField>
                <ParamField path="max_time" type="number">
                    The amount of time in seconds that the query should take maximum.
                </ParamField>
                <ParamField path="variations" type="number">
                    The number of variations to be generated by the model. Must be an integer greater than 0 and less than 50. Default is 1.
                </ParamField>
                <ParamField path="presence_penalty" type="number">
                    The presence penalty to be used by the model. Must be a number between -2 and 2. Default is 0.
                </ParamField>
                <ParamField path="seed" type="number">
                    The seed to be used by the model. Must be an integer or null.
                </ParamField>
                <ParamField path="stop" type="array">
                    An array of strings to be used as stop sequences by the model. Must be an array of strings with a maximum of 4 strings or null.
                </ParamField>
                <ParamField path="temperature" type="number">
                    The temperature to be used by the model. Must be a number between 0 and 2. Default is 1.
                </ParamField>
                <ParamField path="top_k" type="number">
                    The number of highest probability vocabulary tokens to keep for nucleus sampling. Must be a number between 0 and 1 or null. Default is 1.
                </ParamField>
                <ParamField path="top_p" type="number">
                    The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be a number between 0 and 1 or null. Default is 1.
                </ParamField>
                <ParamField path="logprobs" type="boolean">
                    Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message
                </ParamField>
                <ParamField path="top_logprobs" type="number">
                    An integer specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.
                </ParamField>
                <ParamField path="return_full_text" type="boolean">
                    Whether to have original query in the generated response.
                </ParamField>
                <ParamField path="do_sample" type="boolean">
                    Whether or not to use sampling, use greedy decoding otherwise.
                </ParamField>

            </Expandable>
        </ParamField>
    </Expandable>
</ParamField>

<ParamField body="prompt" type="Object">
  Contains the prompt data to be used in the request. Required if query is not present.
  <Expandable title="properties">
    <ParamField path="prompt_id" type="string" required="true">
      Prompt identifier.
    </ParamField>
    <ParamField path="prompt_variant_id" type="string">
      Prompt variant identifier.
    </ParamField>
    <ParamField path="context" type="Object">
      Dictionary of context variables.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="query" type="Object">
  Contains the query to be processed by the model.
  <Expandable title="properties">
    <ParamField path="content" type="string" required="true">
      The content of the message.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="response" type="Object" required="true">
  Defines the desired response format and options.
  <Expandable title="properties">
    <ParamField path="format" type="string">
      The format of the response. Must be one of `text` or `json_object`.
      Default is `text`.
    </ParamField>
    <ParamField path="stream" type="boolean">
      Whether the response should be streamed. Default is `false`.
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="options" type="Object">
  Contains any additional parameters to be used by Gateway API.
</ParamField>

## Completion Response

This response will contain the model's response to the completion request.

<ResponseField name="tryId" type="string">
  The unique identifier of the completion request.
</ResponseField>

<ResponseField name="status" type="Object">
  The status of the completion response.
  <Expandable title="properties">
    <ResponseField name="type" type="string">
      The type of the status. Can be one of `ok`, `platform_error`, `error`.
    </ResponseField>
    <ResponseField name="code" type="number">
        - `200` - The request was successful.
        - `400` - The request was invalid.
        - `401` - The request was unauthorized.
        - `611` - The request was forbidden.
        - `201` - The request was not found.
        - `600` - The request failed due to an internal server error.
        - `603` - The request failed due to a service unavailable error.
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="model" type="Object">
  The model used for the completion response.
  <Expandable title="properties">
    <ResponseField name="platform" type="string">
      The platform of the model.
    </ResponseField>
    <ResponseField name="name" type="string">
      The name of the model.
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="data" type="Object">
  The data of the completion response.
</ResponseField>

<ResponseField name="usage" type="Object">
  The usage of the completion response.
</ResponseField>

<ResponseField name="meta" type="Object">
  The meta information of the completion response.
</ResponseField>

<ResponseField name="retries" type="array">
  An array of retry objects.
  <Expandable title="properties">
    <ResponseField name="id" type="string">
      The unique identifier of the retry.
    </ResponseField>
    <ResponseField name="platform" type="string">
      The platform of the retry.
    </ResponseField>
    <ResponseField name="name" type="string">
      The name of the retry.
    </ResponseField>
    <ResponseField name="error" type="string">
      The error message of the retry.
    </ResponseField>
    <ResponseField name="timeout" type="number">
      The timeout of the retry.
    </ResponseField>
  </Expandable>
</ResponseField>

<RequestExample>
```bash Curl

curl --request POST \
--url https://gateway.zella.ai/v1/completions \
--header 'content-type: application/json' \
--header 'Authorization: Bearer api_h8j3g7cc68a89sc2eod' \
--data '{
"user": "user_jskjf93o101",
"model": {
    "platform": "openai",
    "name": "gpt-3.5-turbo"
},
"query": {
    "content": "Who was Darth Vader?",
},
"response": {
    "format": "json_object",
    "stream": true
    }
}'

````

```python Python

# Create Request Parameters
user = "user_jskjf93o101"
model = {
    "platform": "openai",
    "name": "gpt-3.5-turbo"
}
query = {
    "content": "Who was Darth Vader?"
}
response = {
    "format": "json_object",
    "stream": True
}

# Call API
stream = zella_ai.completions.create(user, model, query, response)

# Iterate over stream
for chunk in stream:
    if chunk:
        print(chunk)
````

</RequestExample>

The example shows how to make a POST request to the Completions API using curl and Python. The API requires a json body with the properties `user`, `model`, `query`, and `response`.

If the request is successful, the API will return a json object containing the model's response to the completion request. If there is an error, the API will return a json object containing details about the error.
