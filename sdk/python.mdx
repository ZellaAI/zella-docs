---
title: 'Python'
icon: 'python'
---

To get started, create an account on [Zella AI](https://zella.ai/) and get your Zella access token.

Once you have that all set up, [install Zella Python SDK using](https://pypi.org/project/zella/) `pip`.

```bash
pip install zella
```

Zella Python library has support for both Zella Gateway as well as Langchain callbacks.

## AI Gateway

Zella's AI Gateway, paired with our Python SDK, offers a simple and efficient way to integrate AI
features into your applications. It acts as a bridge, connecting Python developers to a diverse
range of AI tools and services through one streamlined platform.

### Chat Completion

Connect to the Zella AI Chat Completion API using our Python SDK.

<CodeGroup>

```python chat.py
from zella import ZellaAI

zella_ai = ZellaAI(api_key="*****************")

user = "2312re3r33e33"
model = {
    "name": "gpt-3.5-turbo"
}
query = {
    "messages": [ {
        "role": "system",
        "content": "You are a helpful assistant to help design api structure"
    },
    {
        "role": "user",
        "content": "Hi There!"
    }]
}
response = zella_ai.chat.completions.create(user=user, model=model, query=query)
```

</CodeGroup>

### Chat Completion with streaming

To enable streaming responses, simply set the 'stream' attribute to True in the response object.
This feature is available only if the underlying model platform supports streaming. Below is a
sample implementation:

<CodeGroup>

```python chat_stream.py
from zella import ZellaAI

zella_ai = ZellaAI(api_key="*****************")

user = "2312re3r33e33"
model = {
    "name": "gpt-3.5-turbo"
}
query = {
    "messages": [ {
        "role": "system",
        "content": "You are a helpful assistant to help design api structure"
    },
    {
        "role": "user",
        "content": "Hi There!"
    }]
}
response = {
    "stream": True
}
response = zella_ai.chat.completions.create(user=user, model=model, query=query, response=response)
```

</CodeGroup>


### Chat Completion with Single turn function calling

Here is how to utilise LLM's functioncal calling ability. Currently only OpenAI and Google Gemini is supported for function calling:


<CodeGroup>

```python single_turn_function.py
from zella import ZellaAI

zella_ai = ZellaAI(api_key="*****************")

model = {
    "name": "gemini-pro"
}
query = {
    "messages": [
        {
        "role": "user",
        "content": "How many goals did Ronaldo score in 2022 FIFA World Cup?"
        }
    ]
}
response = {
    "tools": [
        {
            "type": "function",
            "function": {
                "name": "get_world_cup_goals",
                "description": "Get World Cup goals of any player during a given year.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "year": {
                            "type": "string",
                            "description": "The year of world cup"
                        },
                        "player": {
                            "type": "string",
                            "description": "The name of the player"
                        }
                    }
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_world_cup_goals_team",
                "description": "Get World Cup goals of any team during a given year.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "year": {
                            "type": "string",
                            "description": "The year of world cup"
                        },
                        "team": {
                            "type": "string",
                            "description": "The name of the team"
                        }
                    }
                }
            }
        }
    ],
    "preferred_tool": {
        "type": "function",
        "function": {
            "name":"get_world_cup_goals"
        }
    }
}
response = zella_ai.chat.completions.create(model=model, query=query, response=response)
```

```python multi_turn_function.py
from zella import ZellaAI

zella_ai = ZellaAI(api_key="*****************")

model = {
    "name": "gemini-pro"
}
query = {
    "messages": [
        {
            "role": "user",
            "content": "How many goals did Ronaldo score in 2022 FIFA World Cup?"
        },
        {
            "role": "assistant",
            "tool_calls": [
                {
                    "id": "call_A3OsZgm7LQkIORtW6yiJXmpf",
                    "type": "function",
                    "function": {
                        "name": "get_world_cup_goals",
                        "arguments": "{\"player\":\"Ronaldo\",\"year\":\"2022\"}"
                    }
                }
            ]
        },
    {
        "role": "tool",
        "content": "{\"goals\": 10}",
        "tool_call_id": "call_A3OsZgm7LQkIORtW6yiJXmpf"
    }
    ]
}
response = {
    "tools": [
        {
            "type": "function",
            "function": {
                "name": "get_world_cup_goals",
                "description": "Get World Cup goals of any player during a given year.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "year": {
                            "type": "string",
                            "description": "The year of world cup"
                        },
                        "player": {
                            "type": "string",
                            "description": "The name of the player"
                        }
                    }
                }
            }
        },
        {
            "type": "function",
            "function": {
                "name": "get_world_cup_goals_team",
                "description": "Get World Cup goals of any team during a given year.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "year": {
                            "type": "string",
                            "description": "The year of world cup"
                        },
                        "team": {
                            "type": "string",
                            "description": "The name of the team"
                        }
                    }
                }
            }
        }
    ],
    "preferred_tool": {
        "type": "function",
        "function": {
            "name":"get_world_cup_goals"
        }
    }
}
response = zella_ai.chat.completions.create(model=model, query=query, response=response)
```

</CodeGroup>

### Embeddings

Transform text into insightful vectors with Zella,
utilizing a range of embedding models and platforms for diverse applications.

<CodeGroup>

```python embedding.py
from zella import ZellaAI

zella_ai = ZellaAI(api_key="*****************")

user = "2312re3r33e33"
model = {
    "name": "text-embedding-ada-002"
}
query = {
    "input": "Hello World!",
}
response = {
    "format": "float"
}
response = zella_ai.embedding.embed(user=user, model=model, query=query, response=response)
```

</CodeGroup>


### Push Messages

Push messages to Zella for analytics and evaluation.

<CodeGroup>

```python log.py
from zella import ZellaAI

zella_ai = ZellaAI(api_key="*************")

input = [{
    "role": "system",
    "content": "You are a maths teacher!"
}, {
    "role": "user",
    "content": "What is 2+2?"
}]
output = "4."
model = {
    "platform": "openai",
    "name": "gpt-4"
}

response = zella_ai.logger.log(input, output, model, **{"message_chain_id": "mc_localtest", "tags": ["test"], "user": "test-user"})
```

</CodeGroup>